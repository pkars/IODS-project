# Clustering and classification
A new week, a new section! This week is about clustering the data. As per usual, here are the libraries I'm using.
```{r 'ch4-setup', message=FALSE, echo=-1:-5}
# knitr is used to build this report, so it is included by default

# Remove the '##' from each line of R output blocks
knitr::opts_chunk$set(comment = '')

# Used knitr options: No "##" before output
library(dplyr)
library(ggplot2)
library(GGally)
library(gridExtra)
library(reshape2)
library(tidyr)
library(MASS)  # Contains the used dataset
library(corrplot)
set.seed(2020)
```

## Data description
Unlike previous weeks, this week we are using a dataset that we have not wrangled with before! The data is the `Boston` dataset that is available in R package called `MASS`. Below is a table that lists the columns in the dataset. Full disclosure, I lifted the descriptions directly from R documentation (accessible by typing `?Boston` in R console).

| Column | Description |
|-|-------|
|crim   | per capita crime rate by town.|
|zn     | proportion of residential land zoned for lots over 25,000 sq.ft.|
|indus  | proportion of non-retail business acres per town.|
|chas   | Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).|
|nox    | nitrogen oxides concentration (parts per 10 million).|
|rm     | average number of rooms per dwelling.|
|age    | proportion of owner-occupied units built prior to 1940.|
|dis    | weighted mean of distances to five Boston employment centres.|
|rad    | index of accessibility to radial highways.|
|tax    | full-value property-tax rate per \$10,000.|
|ptratio| pupil-teacher ratio by town.|
|black  | *1000(Bk - 0.63)^2* where *Bk* is the proportion of blacks by town.|
|lstat  | lower status of the population (percent).|
|medv   | median value of owner-occupied homes in \$1000s. |

The data is titled as *Housing Values in Suburbs of Boston*. The dataset contains 14 variables and 506 rows. All columns are numerical, although the variable `chas` is actually a boolean indicator represented as ones and zeros. Also as per usual, here are the outputs for `str`, `head` and `summary` functions for the `Boston` dataset:

```{r 'ch4-task2'}
data("Boston")  # Creates variable named Boston
str(Boston)
knitr::kable(head(Boston, 5))
summary(Boston)
```
Looking at the summary, there are a couple of notable elements visible. First, the `crim` (*per capita crime rate by town*) variable is under 4 for most of the rows, but the maximum is *88.98*, almost three magnitudes larger than the median value (0.26)! The high discrepancy could be explained by the town in question having a low number of people, however the data doesn't include this value. The variable `zn` (*proportion of residential land zoned for lots over 25000 sq.ft.*) exhibits similar behavior: minimum and median values are both 0 and maximum value is 100. This tells that most of the towns have no residential lots over 25000 sq.ft. in size. Similarly for variable `chas`, which simply denotes which areas are connected to Charles River and which not. Data states that at least 75% of the data is not connected to the river. For drawing more insight from the rest of the data, let's plot a couple of pictures.

```{r 'ch4-task3', message=FALSE}
ggpairs(Boston)
```

Okay, so, the `ggpairs` plot out-of-the-box isn't the most clearest presentation. Let's try some alternatives... First, let's take a look at the density plots for each variable. (I finally figured out how to do this, thanks StackOverflow! Yay, go me.) The only variable resembling a normal distribution is `rm`, which is *average number of rooms per dwelling*. Okay, makes sense as it is an average value. Variables `indus`, `rad` and `tax` show two peaks, meaning that there are more towns with "low" and "high" values than "middle" values. The populace seems to be elderly, as the density increases (almost monotonically) as `age` gets larger. Variables `nox`, `dis`, `lstat` and `medv` are positively skewed and `ptratio` is negatively skewed.

```{r message=FALSE}
longtable <- melt(Boston)
ggplot(longtable, aes(value)) + geom_density() + facet_wrap(~variable, scales="free")
```

Interestingly, the variable `black` looks like a mirrored version of `crim`. It's a derived value related to the proportion of black people. I don't know how the original dataset authors derived the function ($1000\left(Bk - 0.63\right)^2$), but the density graph shows that most of the towns have a proportion of black people that deviates from value 0.63. Plotting the function shows that when its value is over 375.38 (i.e. 1st quantile), the proportion of black people is under 0.02.

```{r}
x <- 0:100 / 100
y <- 1000*(x-0.63)^2
plot(x,y, xlab="Proportion of black people", ylab="Function value")
# Show the values of x when y > 375.38 (i.e. the value of 1st quantile)
x[y > 375.38]
```

Finally, let's plot the correlation between all variables using the fancy `corrplot` function from the `corrplot` library. From the picture, looks like `rad` and `tax` are strongly correlated (positive). That implies that the towns closer to more highways also tax their people more. Other notable correlations are `indus`, `nox` and `age` each negatively correlating with `dis`, and `lstat` correlating with `medv` negatively. Notably, the further away from an employment hotspot the town is, the less there is industrial activity and less pollution (indicated by amount of NO_x_) there is.

```{r message=FALSE}
corrplot(cor(Boston), method="circle")
```

## Data wrangling
Whew, that was a lot of text for simply exploring the data! Now I have to actually do something with it, so let's get to it! First, the data needs to be standardized, which is done by subtracting the mean from the values and dividing by the standard deviation for each column. Now the data has a mean value of zero for each column! Also, the values are now "how many standard deviations away they are from the mean value" instead of actual values. However, the density functions' form *did not* change, which is also what we want.

```{r 'ch4-task4-1'}
scaled <- scale(Boston)

# scale() returns a matrix object, change it to a data.frame
scaled <- as.data.frame(scaled)
summary(scaled)

longtable <- melt(scaled)
ggplot(longtable, aes(value)) + geom_density() + facet_wrap(~variable, scales="free")
```

In this exercise, we're trying to ultimately predict the crime rate using Linear Discriminant Analysis (LDA). To do this, we first create a categorical target variable for crime rate by dividing the data. Here we do this by cutting the data into four quantiles.

```{r 'ch4-task4-2'}
# create a quantile vector of crim and print it
bins <- quantile(scaled$crim)
bins

# create a categorical variable 'crime' and take a look using table()
crime <- cut(scaled$crim, breaks = bins, include.lowest = TRUE, label=c("low", "med_low", "med_high", "high"))
table(crime)

# remove original `crim` and add the new `crime` variable to the dataset
# Here I need to explicitly call dplyr::select, as MASS library also has a select function
scaled <- dplyr::select(scaled, -crim)
scaled <- data.frame(scaled, crime)
```

Next up, we'll have to train and test the LDA model with our data. So, let's split the data to training and testing datasets using a 80:20 split. Also, the test data shouldn't contain the true labels so that we don't confuse our model, but we need that information for verifying our model. So let's store the labels into another object.
```{r 'ch4-task4-3'}
# number of rows in the Boston dataset 
n <- nrow(scaled)

# choose randomly 80% of the rows
ind <- sample(n,  size = n * 0.8)

# create train and test sets
train <- scaled[ind,]
test <- scaled[-ind,]  # note the minus!

# save the correct classes from test data
correct_classes_for_test <- test$crime

# remove the crime variable from test data
test <- dplyr::select(test, -crime)
```

## Linear Discriminant Analysis

Now that our data is in a suitable format, let's get to business. Luckily, fitting the LDA is a one-liner. Here our target is `crime` and explanatory variables are everything else (denoted with `.`). Also, I shamelessly copied the biplot plotting code from Datacamp exercises (because programming it would be too difficult at this point). Let's see what happens.
```{r 'ch4-task5'}
lda.fit <- lda(crime ~ ., data = train)
lda.fit  # Just checking the output

# Function lifted directly from DataCamp exercises
# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

# target classes as numeric
classes <- as.numeric(train$crime)

# plot the lda results
plot(lda.fit, dimen = 2, col=classes, pch=classes)
lda.arrows(lda.fit, myscale = 1)
```

Okay, that's a colorful mess. Looking at the arrows, `rad` stands out quite nicely. Also printing the fit shows that it has the largest coefficients for linear discriminants LD1 and LD2. Other notable variables would be `zn` and `nox` with high-ish coefficients for LD2 and LD3. Also, the `crime==high` points all are nicely together in the right side of the picture, though some `med_high`s have ended up there, too.

So, what about our model's predictions? Predict with test data and cross tabulate the predictions with the actual labels, got it!
```{r 'ch4-task6'}
# predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)
table(correct = correct_classes_for_test, predicted = lda.pred$class) %>% addmargins
```

Well, looks like the model is successful in predicting the crime rate classes - up to a point. It is a good sign that the diagonal elements have the largest values, as these elements show the correct predictions! However, looking at the numbers in a more detail shows interesting things. The `high` looks quite easy to predict, model correctly predicted 19/20 rows with no false positives, missing one row. For other classes, model didn't perform as well. For `med_high` model had worst performance, predicting correctly only 12/22 with 8 false positives. However, `med_low` had more false positives even if the model also had more rows correctly (20/30 correct, 19 false positives -- that's almost 50% of the predictions in this class)! Finally, 18/30 rows of `low` class were predicted correctly, with "only" 6 false positives.

| Predicted | Correct | Misses | False positives |
|--         |--       |--      |--   |
|low        | 18/30 (60%) | 12/30  | 6/24 (25%)   |
|med_low    | 20/30 (67%) | 10/30  | 19/39 (49%) |
|med_high   | 12/22 (55%) | 10/22  | 8/20 (40%)  |
|high       | 19/20 (95%) |  1/20  | 0/19 (0%)|

## k-means Clustering
Finally, we'll practice some clustering with the "clustering hello world algorithm", also known as k-means clustering. Basically this algorithm creates *k* clusters by finding *k* points that minimize the mean distance between the "k-points" and the data points (though it's not foolproof, the outcome depends on where the initial k-points are). Each data point is assigned to the nearest k-point, which forms the cluster.

Moving on, now that short (and probably a bit unnecessary) introduction is out of the way, I'll reload the original `Boston` dataset and scale it once again for clustering.
```{r 'ch4-task7-1'}
# Reload original Boston dataset
data('Boston')

# Scale the dataset
boston_scaled <- scale(Boston) %>% as.data.frame()
# I won't repeat the summary here, it's the same as shown previously...

# Calculate euclidean distance and show it
dist_eu <- dist(boston_scaled)
summary(dist_eu)
 
# Let's calculate also manhattan distance for good measure
dist_man <- dist(boston_scaled, method="manhattan")
summary(dist_man)
```

Okay, next up is the actual clustering. Let's start with three k-points and see the result.

```{r 'ch4-task7-2'}
km <-kmeans(boston_scaled, centers = 3)
# Plot a subset of boston_scaled data with clusters colored, just to see how it looks like
pairs(boston_scaled[5:9], col = km$cluster)
```

I don't know whose idea it was to have black, *red* and *green* as the three first default colors in R... I just hope that these shades of red and green are distinct enough for (red-green) colorblind people are able to tell the difference. And wow, the clusters swap colors when I replot the figure! Yeah, I really have to learn the `ggplot2` library by heart if I'm ever going to use R outside this course. But I digress. Moving on...

Multiple methods for figuring out the optimal value for *k* exist, but here we use a metric called *total of within cluster sum of squares* (WCSS). The optimal number for k is where the WCSS has a ***radical*** drop! The `kmeans` function returns an object that handily provides the WCSS value in `tot.withinss` component. The DataCamp exercises graciously include an one-liner to calculate WCSS for the data using `k=1...k_max`, so I'm totally going to use that one! Let's set `k_max` to 10 to limit the iterations a bit and then we'll look for the optimal value visually - i.e by plotting `WCSS(k)`.

```{r 'ch4-task7-3', message=FALSE}
# Maximum number of clusters considered here
k_max <- 10

# This one is also copied from DataCamp...
# calculate the total within sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(boston_scaled, k)$tot.withinss})

# Visualise!
qplot(x = 1:k_max, y = twcss, geom = c('line', 'point'))
```

The most ***radical*** decrease would be from going `k=1` to `k=2`, so the value 2 seems to be a great candidate. Though the WCSS values drop up to `k=9`, so something like 5 or 7 might be potential candidates. But 2 makes it so that there will be only two colors in the up-coming plots, making them a bit easier to interpret. So I'll use `k=2`.[^3]

```{r 'ch4-task7-4', message=FALSE, warning=FALSE}
# Re-cluster the data
km <-kmeans(boston_scaled, centers = 2)
# 
# plot the Boston dataset with clusters
# This time the full dataset with ggpairs -- it's stil going to be a mess, but it'll be a pretty mess.
ggpairs(boston_scaled, aes(col=as.factor(km$cluster)))
# This chunk emits a bunch of "Warning in cor(x, y): the standard deviation is zero" warning messages.
```

Yep, that's a pretty looking mess. Also that chunk emits a bunch of `Warning in cor(x, y): the standard deviation is zero` warning messages (I've omitted those as they're wasting space). Looks like the correlations for `zn` in the red class are `NA`, so that's probably where the standard deviation is zero. Anyways, let's first plot the density plots like before, and then check only a handful of variables with larger pictures.

```{r 'ch4-task7-5', message=FALSE}
with_colors <- boston_scaled
with_colors$cluster <- as.factor(km$cluster)
longtable <- melt(with_colors)
ggplot(longtable, aes(value, col=cluster)) + geom_density() + facet_wrap(~variable, scales="free")
```

Well, that's interesting! Some of the variables now show two differing density curves -- essentially meaning that there could be a way to separate the data into two different groups. If you recall my pondering from the data exploration, I noted that `indus`, `rad` and `tax` variables show two peaks -- well, the two peaks seem to belong into different clusters! However, what I couldn't see before, was that also `age`, `dis`, `ptratio`, and arguably `lstat` and `medv`, would be a combination of two different (and separable) curves. `nox` being also very separable is not a surprise (in hindsight), when you already know that industrial zones cause more pollution than residential zones (typically).

Let's pick some variables that look interesting.
```{r message=FALSE, warning=FALSE}
#with_colors <- boston_scaled
#with_colors$cluster <- as.factor(km$cluster)
subset <- dplyr::select(with_colors, c("indus", "age", "dis", "rad", "tax", "medv", "cluster"))
#subset$cluster <- as.factor(subset$cluster)
ggpairs(subset, aes(col=cluster))
```

Well, they look interesting. For example, `tax` looks to be separated quite nicely, same with `dis` and `age`. Old people live close to the industrial employment centers and pay more in taxes? That would be one plausible conclusion from this data.

## Bonus task
Surprise! There are two bonus tasks for this week! There were some also for the last week, but I didn't do those. But it's getting a bit late, so this'll have to be short. The first one is to fit LDA for some number of clusters (>2). I chose 7. For that, the most influential variables seem to be `chas` and `rad`. The second bonus task, well, I can't manage everything.
```{r 'ch4-bonus'}
# Reload original Boston dataset
data('Boston')

# Scale the dataset
bonus_scaled <- scale(Boston) %>% as.data.frame
summary(bonus_scaled)

# k-means clustering with 7 clusters
km <-kmeans(bonus_scaled, centers = 7)
summary(km)

# Add clusters to dataset
bonus_scaled$cluster <- km$cluster

# LDA fit targetting clusters
lda.fit <- lda(cluster ~ ., data = bonus_scaled)
lda.fit  # Just checking the output

# target classes as numeric
classes <- as.numeric(bonus_scaled$cluster)

# plot the lda results
plot(lda.fit, dimen = 2, col=classes, pch=classes)
lda.arrows(lda.fit, myscale = 1)
```

[^3]: I also got `k=3` as optimal value when I accidentally ran the code without setting seed beforehand. Whoops. Well, like I said, the results of k-means depend on the initial (random) points.

# Clustering and classification

```{r 'ch4-setup', message=FALSE, echo=-1:-5}
# knitr is used to build this report, so it is included by default

# Remove the '##' from each line of R output blocks
knitr::opts_chunk$set(comment = '')

# Used knitr options: No "##" before output
library(dplyr)
library(ggplot2)
library(GGally)
library(gridExtra)
library(tidyr)
library(MASS)  # Contains the used dataset
library(corrplot)
```

## Data description
**Load the Boston data from the MASS package. Explore the structure and the dimensions of the data and describe the dataset briefly, assuming the reader has no previous knowledge of it. Details about the Boston dataset can be seen for example here. (0-1 points)**
```{r 'ch4-task2'}
# Task 2
data("Boston")  # Creates variable named Boston
str(Boston)
knitr::kable(head(Boston, 5))
```

Table showing the data columns, lifted directly from R documentation (type `?Boston` in R console).

| Column | Description |
|-|-------|
|crim   | per capita crime rate by town.|
|zn     | proportion of residential land zoned for lots over 25,000 sq.ft.|
|indus  | proportion of non-retail business acres per town.|
|chas   | Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).|
|nox    | nitrogen oxides concentration (parts per 10 million).|
|rm     | average number of rooms per dwelling.|
|age    | proportion of owner-occupied units built prior to 1940.|
|dis    | weighted mean of distances to five Boston employment centres.|
|rad    | index of accessibility to radial highways.|
|tax    | full-value property-tax rate per \$10,000.|
|ptratio| pupil-teacher ratio by town.|
|black  | 1000(*Bk* - 0.63)^2 where *Bk* is the proportion of blacks by town.|
|lstat  | lower status of the population (percent).|
|medv   | median value of owner-occupied homes in \$1000s. |

```{}
Source

Harrison, D. and Rubinfeld, D.L. (1978) Hedonic prices and the demand for clean air. J. Environ. Economics and Management 5, 81â€“102.

Belsley D.A., Kuh, E. and Welsch, R.E. (1980) Regression Diagnostics. Identifying Influential Data and Sources of Collinearity. New York: Wiley. 
```

**Show a graphical overview of the data and show summaries of the variables in the data. Describe and interpret the outputs, commenting on the distributions of the variables and the relationships between them. (0-2 points)**
```{r 'ch4-task3'}
# Task 3
summary(Boston)
pairs(Boston)
corrplot(cor(Boston), method="circle")
```

## Data wrangling
**Standardize the dataset and print out summaries of the scaled data. How did the variables change?**
Standardization is done by subtracting the mean from the values and dividing by the standard deviation for each column.
```{r 'ch4-task4-1'}
boston_scaled <- scale(Boston)
#summary(boston_scaled)
# scale() returns a matrix object, change it to a data.frame
boston_scaled <- as.data.frame(boston_scaled)
summary(boston_scaled)
```
**Create a categorical variable of the crime rate in the Boston dataset (from the scaled crime rate). Use the quantiles as the break points in the categorical variable. Drop the old crime rate variable from the dataset. **
```{r 'ch4-task4-2'}
# # create a quantile vector of crim and print it
# bins <- quantile(boston_scaled$crim)
# bins
# 
# # create a categorical variable 'crime'
# crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, label=c("low", "med_low", "med_high", "high"))
# 
# # look at the table of the new factor crime
# table(crime)
# 
# # remove original crim from the dataset
# boston_scaled <- dplyr::select(boston_scaled, -crim)
# 
# # add the new categorical value to scaled data
# boston_scaled <- data.frame(boston_scaled, crime)

# # boston_scaled is available

```

**Divide the dataset to train and test sets, so that 80% of the data belongs to the train set. (0-2 points)**
```{r 'ch4-task4-3'}

# # number of rows in the Boston dataset 
# n <- nrow(boston_scaled)
# 
# # choose randomly 80% of the rows
# ind <- sample(n,  size = n * 0.8)
# 
# # create train set
# train <- boston_scaled[ind,]
# 
# # create test set 
# test <- boston_scaled[-ind,]
# 
# # save the correct classes from test data
# correct_classes <- test$crime
# 
# # remove the crime variable from test data
# test <- dplyr::select(test, -crime)
# 
```

**Fit the linear discriminant analysis on the train set. Use the categorical crime rate as the target variable and all the other variables in the dataset as predictor variables. Draw the LDA (bi)plot. (0-3 points)**
```{r 'ch4-task5'}
# # linear discriminant analysis
# lda.fit <- lda(crime ~ ., data = train)
# 
# # print the lda.fit object
# lda.fit
# 
# Function lifted directly from DataCamp exercises
# # the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}
# 
# # target classes as numeric
# classes <- as.numeric(train$crime)
# 
# # plot the lda results
# plot(lda.fit, dimen = 2, col=classes, pch=classes)
# lda.arrows(lda.fit, myscale = 1)
```

**Save the crime categories from the test set and then remove the categorical crime variable from the test dataset. Then predict the classes with the LDA model on the test data. Cross tabulate the results with the crime categories from the test set. Comment on the results. (0-3 points)**
```{r 'ch4-task6'}
# # predict classes with test data
# lda.pred <- predict(lda.fit, newdata = test)
# 
# # cross tabulate the results
# table(correct = correct_classes, predicted = lda.pred$class)
```

**Reload the Boston dataset and standardize the dataset (we did not do this in the Datacamp exercises, but you should scale the variables to get comparable distances). Calculate the distances between the observations. Run k-means algorithm on the dataset. Investigate what is the optimal number of clusters and run the algorithm again. Visualize the clusters (for example with the pairs() or ggpairs() functions, where the clusters are separated with colors) and interpret the results. (0-4 points)**
```{r 'ch4-task7'}
# Reload original Boston dataset
data('Boston')

# Scale the dataset
boston_scaled <- scale(Boston)
# 
# # euclidean distance matrix
# dist_eu <- dist(Boston)
# 
# # look at the summary of the distances
# summary(dist_eu)
# 
# # manhattan distance matrix
# dist_man <- dist(Boston, method="manhattan")
# 
# # look at the summary of the distances
# summary(dist_man)
# # 
# # Boston dataset is available
# 
# # k-means clustering
# km <-kmeans(Boston, centers = 3)
# 
# # plot the Boston dataset with clusters
# pairs(Boston[6:10], col = km$cluster)
# 
# # MASS, ggplot2 and Boston dataset are available
# set.seed(123)
# 
# # determine the number of clusters
# k_max <- 10
# 
# # calculate the total within sum of squares
# twcss <- sapply(1:k_max, function(k){kmeans(Boston, k)$tot.withinss})
# 
# # visualize the results
# qplot(x = 1:k_max, y = twcss, geom = 'line')
# 
# # k-means clustering
# km <-kmeans(Boston, centers = 2)
# 
# # plot the Boston dataset with clusters
# pairs(Boston, col = km$cluster)
# 

```

**Bonus: Perform k-means on the original Boston data with some reasonable number of clusters (> 2). Remember to standardize the dataset. Then perform LDA using the clusters as target classes. Include all the variables in the Boston data in the LDA model. Visualize the results with a biplot (include arrows representing the relationships of the original variables to the LDA solution). Interpret the results. Which variables are the most influencial linear separators for the clusters? (0-2 points to compensate any loss of points from the above exercises)**
```{r 'ch4-bonus'}
# Reload original Boston dataset
data('Boston')
# Scale the dataset
boston_scaled <- scale(Boston)
# # # k-means clustering
# km <-kmeans(Boston, centers = 3)
```

**Super-Bonus: Run the code below for the (scaled) train data that you used to fit the LDA. The code creates a matrix product, which is a projection of the data points.**
```{r}

# model_predictors <- dplyr::select(train, -crime)
# 
# # check the dimensions
# dim(model_predictors)
# dim(lda.fit$scaling)
# 
# # matrix multiplication
# matrix_product <- as.matrix(model_predictors) %*% lda.fit$scaling
# matrix_product <- as.data.frame(matrix_product)
```
**Next, install and access the plotly package. Create a 3D plot (Cool!) of the columns of the matrix product by typing the code below.**

```{r}
# plot_ly(x = matrix_product$LD1, y = matrix_product$LD2, z = matrix_product$LD3, type= 'scatter3d', mode='markers')
```

**Adjust the code: add argument color as a argument in the plot_ly() function. Set the color to be the crime classes of the train set. Draw another 3D plot where the color is defined by the clusters of the k-means. How do the plots differ? Are there any similarities? (0-3 points to compensate any loss of points from the above exercises)**
```{r 'ch4-super-bonus'}
```


```{r 'ch4-stash', eval=FALSE, include=FALSE}
# # calculate the correlation matrix and round it
# cor_matrix<-cor(Boston) 
# 
# # print the correlation matrix
# cor_matrix <- cor_matrix %>% round(digits=2)
# cor_matrix
# 
# # visualize the correlation matrix
# corrplot(cor_matrix, method="circle", type="upper",
# cl.pos="b", tl.pos="d", tl.cex=0.6)
# 
# 
# # MASS and Boston dataset are available
# 
# # center and standardize variables
# boston_scaled <- scale(Boston)
# 
# # summaries of the scaled variables
# summary(boston_scaled)
# 
# # class of the boston_scaled object
# class(boston_scaled)
# 
# # change the object to data frame
# boston_scaled <- as.data.frame(boston_scaled)
# 
# 
# # MASS, Boston and boston_scaled are available
# boston_scaled <- as.data.frame(scale(Boston))
# # summary of the scaled crime rate
# summary(boston_scaled)
# 
# # create a quantile vector of crim and print it
# bins <- quantile(boston_scaled$crim)
# bins
# 
# # create a categorical variable 'crime'
# crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, label=c("low", "med_low", "med_high", "high"))
# 
# # look at the table of the new factor crime
# table(crime)
# 
# # remove original crim from the dataset
# boston_scaled <- dplyr::select(boston_scaled, -crim)
# 
# # add the new categorical value to scaled data
# boston_scaled <- data.frame(boston_scaled, crime)
# 
# 
# 
# # boston_scaled is available
# 
# # number of rows in the Boston dataset 
# n <- nrow(boston_scaled)
# 
# # choose randomly 80% of the rows
# ind <- sample(n,  size = n * 0.8)
# 
# # create train set
# train <- boston_scaled[ind,]
# 
# # create test set 
# test <- boston_scaled[-ind,]
# 
# # save the correct classes from test data
# correct_classes <- test$crime
# 
# # remove the crime variable from test data
# test <- dplyr::select(test, -crime)
# 
# 
# # MASS and train are available
# 
# # linear discriminant analysis
# lda.fit <- lda(crime ~ ., data = train)
# 
# # print the lda.fit object
# lda.fit
# 
# # the function for lda biplot arrows
# lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
#   heads <- coef(x)
#   arrows(x0 = 0, y0 = 0, 
#          x1 = myscale * heads[,choices[1]], 
#          y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
#   text(myscale * heads[,choices], labels = row.names(heads), 
#        cex = tex, col=color, pos=3)
# }
# 
# # target classes as numeric
# classes <- as.numeric(train$crime)
# 
# # plot the lda results
# plot(lda.fit, dimen = 2, col=classes, pch=classes)
# lda.arrows(lda.fit, myscale = 1)
# 
# 
# # lda.fit, correct_classes and test are available
# 
# # predict classes with test data
# lda.pred <- predict(lda.fit, newdata = test)
# 
# # cross tabulate the results
# table(correct = correct_classes, predicted = lda.pred$class)
# 
# 
# # load MASS and Boston
# library(MASS)
# data('Boston')
# 
# # euclidean distance matrix
# dist_eu <- dist(Boston)
# 
# # look at the summary of the distances
# summary(dist_eu)
# 
# # manhattan distance matrix
# dist_man <- dist(Boston, method="manhattan")
# 
# # look at the summary of the distances
# summary(dist_man)
# 
# 
# # Boston dataset is available
# 
# # k-means clustering
# km <-kmeans(Boston, centers = 3)
# 
# # plot the Boston dataset with clusters
# pairs(Boston[6:10], col = km$cluster)
# 
# # MASS, ggplot2 and Boston dataset are available
# set.seed(123)
# 
# # determine the number of clusters
# k_max <- 10
# 
# # calculate the total within sum of squares
# twcss <- sapply(1:k_max, function(k){kmeans(Boston, k)$tot.withinss})
# 
# # visualize the results
# qplot(x = 1:k_max, y = twcss, geom = 'line')
# 
# # k-means clustering
# km <-kmeans(Boston, centers = 2)
# 
# # plot the Boston dataset with clusters
# pairs(Boston, col = km$cluster)
# 


```
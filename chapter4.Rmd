# Clustering and classification
A new week, a new section! This week is about clustering the data. As per usual, here are the libraries I'm using.
```{r 'ch4-setup', message=FALSE, echo=-1:-5}
# knitr is used to build this report, so it is included by default

# Remove the '##' from each line of R output blocks
knitr::opts_chunk$set(comment = '')

# Used knitr options: No "##" before output
library(dplyr)
library(ggplot2)
library(GGally)
library(gridExtra)
library(reshape2)
library(tidyr)
library(MASS)  # Contains the used dataset
library(corrplot)
```

## Data description
Unlike previous weeks, this week we are using a dataset that we have not wrangled with before! The data is the `Boston` dataset that is available in R package called `MASS`. Below is a table that lists the columns in the dataset. Full disclosure, I lifted the descriptions directly from R documentation (accessible by typing `?Boston` in R console).

| Column | Description |
|-|-------|
|crim   | per capita crime rate by town.|
|zn     | proportion of residential land zoned for lots over 25,000 sq.ft.|
|indus  | proportion of non-retail business acres per town.|
|chas   | Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).|
|nox    | nitrogen oxides concentration (parts per 10 million).|
|rm     | average number of rooms per dwelling.|
|age    | proportion of owner-occupied units built prior to 1940.|
|dis    | weighted mean of distances to five Boston employment centres.|
|rad    | index of accessibility to radial highways.|
|tax    | full-value property-tax rate per \$10,000.|
|ptratio| pupil-teacher ratio by town.|
|black  | *1000(Bk - 0.63)^2* where *Bk* is the proportion of blacks by town.|
|lstat  | lower status of the population (percent).|
|medv   | median value of owner-occupied homes in \$1000s. |

The data is titled as *Housing Values in Suburbs of Boston*. The dataset contains 14 variables and 506 rows. All columns are numerical, although the variable `chas` is actually a boolean indicator represented as ones and zeros. Also as per usual, here are the outputs for `str`, `head` and `summary` functions for the `Boston` dataset:

```{r 'ch4-task2'}
data("Boston")  # Creates variable named Boston
str(Boston)
knitr::kable(head(Boston, 5))
summary(Boston)
```
Looking at the summary, there are a couple of notable elements visible. First, the `crim` (*per capita crime rate by town*) variable is under 4 for most of the rows, but the maximum is *88.98*, almost three magnitudes larger than the median value (0.26)! The high discrepancy could be explained by the town in question having a low number of people, however the data doesn't include this value. The variable `zn` (*proportion of residential land zoned for lots over 25000 sq.ft.*) exhibits similar behavior: minimum and median values are both 0 and maximum value is 100. This tells that most of the towns have no residential lots over 25000 sq.ft. in size. Similarly for variable `chas`, which simply denotes which areas are connected to Charles River and which not. Data states that at least 75% of the data is not connected to the river. For drawing more insight from the rest of the data, let's plot a couple of pictures.

```{r 'ch4-task3', message=FALSE}
ggpairs(Boston)
```

Okay, so, the `ggpairs` plot out-of-the-box isn't the most clearest presentation. Let's try some alternatives... First, let's take a look at the density plots for each variable. (I finally figured out how to do this, thanks StackOverflow! Yay, go me.) The only variable resembling a normal distribution is `rm`, which is *average number of rooms per dwelling*. Okay, makes sense as it is an average value. Variables `indus`, `rad` and `tax` show two peaks, meaning that there are more towns with "low" and "high" values than "middle" values. The populace seems to be elderly, as the density increases (almost monotonically) as `age` gets larger. Variables `nox`, `dis`, `lstat` and `medv` are positively skewed and `ptratio` is negatively skewed.

```{r message=FALSE}
longtable <- melt(Boston)
ggplot(longtable, aes(value)) + geom_density() + facet_wrap(~variable, scales="free")
```

Interestingly, the variable `black` looks like a mirrored version of `crim`. It's a derived value related to the proportion of black people. I don't know how the original dataset authors derived the function ($1000\left(Bk - 0.63\right)^2$), but the density graph shows that most of the towns have a proportion of black people that deviates from value 0.63. Plotting the function shows that when its value is over 375.38 (i.e. 1st quantile), the proportion of black people is under 0.02.

```{r}
x <- 0:100 / 100
y <- 1000*(x-0.63)^2
plot(x,y, xlab="Proportion of black people", ylab="Function value")
# Show the values of x when y > 375.38 (i.e. the value of 1st quantile)
x[y > 375.38]
```

Finally, let's plot the correlation between all variables using the fancy `corrplot` function from the `corrplot` library. From the picture, looks like `rad` and `tax` are strongly correlated (positive). That implies that the towns closer to more highways also tax their people more. Other notable correlations are `indus`, `nox` and `age` each negatively correlating with `dis`, and `lstat` correlating with `medv` negatively. Notably, the further away from an employment hotspot the town is, the less there is industrial activity and less pollution (indicated by amount of NO_x_) there is.

```{r message=FALSE}
corrplot(cor(Boston), method="circle")
```

## Data wrangling
Whew, that was a lot of text for simply exploring the data! Now I have to actually do something with it, so let's get to it! First, the data needs to be standardized, which is done by subtracting the mean from the values and dividing by the standard deviation for each column. Now the data has a mean value of zero for each column! Also, the values are now "how many standard deviations away they are from the mean value" instead of actual values. However, the density functions' form *did not* change, which is also what we want.

```{r 'ch4-task4-1'}
scaled <- scale(Boston)

# scale() returns a matrix object, change it to a data.frame
scaled <- as.data.frame(scaled)
summary(scaled)

longtable <- melt(scaled)
ggplot(longtable, aes(value)) + geom_density() + facet_wrap(~variable, scales="free")
```

In this exercise, we're trying to ultimately predict the crime rate using Linear Discriminant Analysis (LDA). To do this, we first create a categorical target variable for crime rate by dividing the data. Here we do this by cutting the data into four quantiles.

```{r 'ch4-task4-2'}
# create a quantile vector of crim and print it
bins <- quantile(scaled$crim)
bins

# create a categorical variable 'crime' and take a look using table()
crime <- cut(scaled$crim, breaks = bins, include.lowest = TRUE, label=c("low", "med_low", "med_high", "high"))
table(crime)

# remove original `crim` and add the new `crime` variable to the dataset
# Here I need to explicitly call dplyr::select, as MASS library also has a select function
scaled <- dplyr::select(scaled, -crim)
scaled <- data.frame(scaled, crime)
```

Next up, we'll have to train and test the LDA model with our data. So, let's split the data to training and testing datasets using a 80:20 split. Also, the test data shouldn't contain the true labels so that we don't confuse our model, but we need that information for verifying our model. So let's store the labels into another object.
```{r 'ch4-task4-3'}
# number of rows in the Boston dataset 
n <- nrow(scaled)

# choose randomly 80% of the rows
ind <- sample(n,  size = n * 0.8)

# create train and test sets
train <- scaled[ind,]
test <- scaled[-ind,]  # note the minus!

# save the correct classes from test data
correct_classes_for_test <- test$crime

# remove the crime variable from test data
test <- dplyr::select(test, -crime)
```

## Linear Discriminant Analysis

Now that our data is in a suitable format, let's get to business. Luckily, fitting the LDA is a one-liner. Here our target is `crime` and explanatory variables are everything else (denoted with `.`). Also, I shamelessly copied the biplot plotting code from Datacamp exercises (because programming it would be too difficult at this point). Let's see what happens.
```{r 'ch4-task5'}
lda.fit <- lda(crime ~ ., data = train)
lda.fit  # Just checking the output

# Function lifted directly from DataCamp exercises
# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

# target classes as numeric
classes <- as.numeric(train$crime)

# plot the lda results
plot(lda.fit, dimen = 2, col=classes, pch=classes)
lda.arrows(lda.fit, myscale = 1)
```

Okay, that's a colorful mess. Looking at the arrows, `rad` stands out quite nicely. Also printing the fit shows that it has the largest coefficients for linear discriminants LD1 and LD2. Other notable variables would be `zn` and `nox` with high-ish coefficients for LD2 and LD3. Also, the `crime==high` points all are nicely together in the right side of the picture, though some `med_high`s have ended up there, too.

So, what about our model's predictions? Predict with test data and cross tabulate the predictions with the actual labels, got it!
```{r 'ch4-task6'}
# predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)
table(correct = correct_classes_for_test, predicted = lda.pred$class) %>% addmargins
```

Well, looks like the model is successful in predicting the crime rate classes - up to a point. It is a good sign that the diagonal elements have the largest values, as these elements show the correct predictions! However, looking at the numbers in a more detail shows interesting things. The `high` looks quite easy to predict, model correctly predicted 19/20 rows with no false positives, missing one row. For other classes, model didn't perform as well. For `med_high` model had worst performance, predicting correctly only 12/22 with 8 false positives. However, `med_low` had more false positives even if the model also had more rows correctly (20/30 correct, 19 false positives -- that's almost 50% of the predictions in this class)! Finally, 18/30 rows of `low` class were predicted correctly, with "only" 6 false positives.

| Predicted | Correct | Misses | False positives |
|--         |--       |--      |--   |
|low        | 18/30 (60%) | 12/30  | 6/24 (25%)   |
|med_low    | 20/30 (67%) | 10/30  | 19/39 (49%) |
|med_high   | 12/22 (55%) | 10/22  | 8/20 (40%)  |
|high       | 19/20 (95%) |  1/20  | 0/19 (0%)|

## k-means Clustering
**Reload the Boston dataset and standardize the dataset (we did not do this in the Datacamp exercises, but you should scale the variables to get comparable distances). Calculate the distances between the observations. Run k-means algorithm on the dataset. Investigate what is the optimal number of clusters and run the algorithm again. Visualize the clusters (for example with the pairs() or ggpairs() functions, where the clusters are separated with colors) and interpret the results. (0-4 points)**

Finally, we'll practice some clustering with the "clustering hello world algorithm", also known as k-means clustering. Basically this algorithm creates *k* clusters by finding *k* points that minimize the mean distance between the "k-points" and the data points (though it's not foolproof, the outcome depends on where the initial k-points are). Each data point is assigned to the nearest k-point, which forms the cluster.

Moving on, now that short (and probably a bit unnecessary) introduction is out of the way, I'll reload the original `Boston` dataset and scale it once again for clustering.
```{r 'ch4-task7-1'}
# Reload original Boston dataset
data('Boston')

# Scale the dataset
boston_scaled <- scale(Boston) %>% as.data.frame()
# I won't repeat the summary here, it's the same as shown previously...

# Calculate euclidean distance and show it
dist_eu <- dist(boston_scaled)
summary(dist_eu)
 
# Let's calculate also manhattan distance for good measure
dist_man <- dist(boston_scaled, method="manhattan")
summary(dist_man)
```



```{r 'ch4-task7-2'}
# k-means clustering
km <-kmeans(boston_scaled, centers = 3)
# 
# # plot the Boston dataset with clusters
# pairs(Boston[6:10], col = km$cluster)
# 

# # determine the number of clusters
k_max <- 10
# 
# # calculate the total within sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(boston_scaled, k)$tot.withinss})
# 
# # visualize the results
qplot(x = 1:k_max, y = twcss, geom = 'line')
# 
# # k-means clustering
# km <-kmeans(Boston, centers = 2)
# 
# # plot the Boston dataset with clusters
pairs(boston_scaled, col = km$cluster)
# 
ggpairs(boston_scaled, aes(col=km$cluster))
```
## Bonus task
**Bonus: Perform k-means on the original Boston data with some reasonable number of clusters (> 2). Remember to standardize the dataset. Then perform LDA using the clusters as target classes. Include all the variables in the Boston data in the LDA model. Visualize the results with a biplot (include arrows representing the relationships of the original variables to the LDA solution). Interpret the results. Which variables are the most influencial linear separators for the clusters? (0-2 points to compensate any loss of points from the above exercises)**
```{r 'ch4-bonus'}
# Reload original Boston dataset
data('Boston')
# Scale the dataset
boston_scaled <- scale(Boston) %>% as.data.frame
summary(boston_scaled)
# # # k-means clustering
km <-kmeans(boston_scaled, centers = 7)
summary(km)
```

## Super-Bonus Task
**Super-Bonus: Run the code below for the (scaled) train data that you used to fit the LDA. The code creates a matrix product, which is a projection of the data points.**
```{r 'ch4-super-bonus-provided'}
# This script is provided in assignment #
model_predictors <- dplyr::select(train, -crime)

# check the dimensions
dim(model_predictors)
dim(lda.fit$scaling)
 
# matrix multiplication
matrix_product <- as.matrix(model_predictors) %*% lda.fit$scaling
matrix_product <- as.data.frame(matrix_product)

# Next, install and access the plotly package. Create a 3D plot (Cool!) of the columns of the matrix product by typing the code below.
# plot_ly(x = matrix_product$LD1, y = matrix_product$LD2, z = matrix_product$LD3, type= 'scatter3d', mode='markers')
```

**Adjust the code: add argument color as a argument in the plot_ly() function. Set the color to be the crime classes of the train set. Draw another 3D plot where the color is defined by the clusters of the k-means. How do the plots differ? Are there any similarities? (0-3 points to compensate any loss of points from the above exercises)**
```{r 'ch4-super-bonus'}
```

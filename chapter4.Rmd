# Clustering and classification
A new week, a new section! This week is about clustering the data. As per usual, here are the libraries I'm using.
```{r 'ch4-setup', message=FALSE, echo=-1:-5}
# knitr is used to build this report, so it is included by default

# Remove the '##' from each line of R output blocks
knitr::opts_chunk$set(comment = '')

# Used knitr options: No "##" before output
library(dplyr)
library(ggplot2)
library(GGally)
library(gridExtra)
library(reshape2)
library(tidyr)
library(MASS)  # Contains the used dataset
library(corrplot)
```

## Data description
Unlike previous weeks, this week we are using a dataset that we have not wrangled with before! The data is the `Boston` dataset that is available in R package called `MASS`. Below is a table that lists the columns in the dataset. Full disclosure, I lifted the descriptions directly from R documentation (accessible by typing `?Boston` in R console).

| Column | Description |
|-|-------|
|crim   | per capita crime rate by town.|
|zn     | proportion of residential land zoned for lots over 25,000 sq.ft.|
|indus  | proportion of non-retail business acres per town.|
|chas   | Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).|
|nox    | nitrogen oxides concentration (parts per 10 million).|
|rm     | average number of rooms per dwelling.|
|age    | proportion of owner-occupied units built prior to 1940.|
|dis    | weighted mean of distances to five Boston employment centres.|
|rad    | index of accessibility to radial highways.|
|tax    | full-value property-tax rate per \$10,000.|
|ptratio| pupil-teacher ratio by town.|
|black  | *1000(Bk - 0.63)^2* where *Bk* is the proportion of blacks by town.|
|lstat  | lower status of the population (percent).|
|medv   | median value of owner-occupied homes in \$1000s. |

The data is titled as *Housing Values in Suburbs of Boston*. The dataset contains 14 variables and 506 rows. All columns are numerical, although the variable `chas` is actually a boolean indicator represented as ones and zeros. Also as per usual, here are the outputs for `str`, `head` and `summary` functions for the `Boston` dataset:

```{r 'ch4-task2'}
data("Boston")  # Creates variable named Boston
str(Boston)
knitr::kable(head(Boston, 5))
summary(Boston)
```
Looking at the summary, there are a couple of notable elements visible. First, the `crim` (*per capita crime rate by town*) variable is under 4 for most of the rows, but the maximum is *88.98*, almost three magnitudes larger than the median value (0.26)! The high discrepancy could be explained by the town in question having a low number of people, however the data doesn't include this value. The variable `zn` (*proportion of residential land zoned for lots over 25000 sq.ft.*) exhibits similar behavior: minimum and median values are both 0 and maximum value is 100. This tells that most of the towns have no residential lots over 25000 sq.ft. in size. Similarly for variable `chas`, which simply denotes which areas are connected to Charles River and which not. Data states that at least 75% of the data is not connected to the river. For drawing more insight from the rest of the data, let's plot a couple of pictures.

```{r 'ch4-task3', message=FALSE}
ggpairs(Boston)
```
Okay, so, the `ggpairs` plot out-of-the-box isn't the most clearest presentation. Let's try some alternatives... First, let's take a look at the density plots for each variable. (I finally figured out how to do this, thanks StackOverflow! Yay, go me.) The only variable resembling a normal distribution is `rm`, which is *average number of rooms per dwelling*. Okay, makes sense as it is an average value. 



```{r message=FALSE}
longtable <- melt(Boston)
ggplot(longtable, aes(value)) + geom_density() + facet_wrap(~variable, scales="free")
```

The variable `black` looks like a mirrored version of `crim`. It's a derived value related to the proportion of black people. I don't know how the original dataset authors derived the function ($1000\left(Bk - 0.63\right)^2$), but the density graph shows that most of the towns have a proportion of black people that deviates from value 0.63. Plotting the function shows that when its value is over 375.38 (i.e. 1st quartile), the proportion of black people is under 0.02.

```{r}
x <- 0:100 / 100
y <- 1000*(x-0.63)^2
plot(x,y, xlab="Proportion of black people", ylab="Function value")
# Show the values of x when y > 375.38 (i.e. the value of 1st quartile)
x[y > 375.38]
```

```{r message=FALSE}
corrplot(cor(Boston), method="circle")
```

## Data wrangling
**Standardize the dataset and print out summaries of the scaled data. How did the variables change?**
Standardization is done by subtracting the mean from the values and dividing by the standard deviation for each column.
```{r 'ch4-task4-1'}
boston_scaled <- scale(Boston)
#summary(boston_scaled)
# scale() returns a matrix object, change it to a data.frame
boston_scaled <- as.data.frame(boston_scaled)
summary(boston_scaled)
```
**Create a categorical variable of the crime rate in the Boston dataset (from the scaled crime rate). Use the quantiles as the break points in the categorical variable. Drop the old crime rate variable from the dataset. **
```{r 'ch4-task4-2'}
# # create a quantile vector of crim and print it
# bins <- quantile(boston_scaled$crim)
# bins
# 
# # create a categorical variable 'crime'
# crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, label=c("low", "med_low", "med_high", "high"))
# 
# # look at the table of the new factor crime
# table(crime)
# 
# # remove original crim from the dataset
# boston_scaled <- dplyr::select(boston_scaled, -crim)
# 
# # add the new categorical value to scaled data
# boston_scaled <- data.frame(boston_scaled, crime)

# # boston_scaled is available

```

**Divide the dataset to train and test sets, so that 80% of the data belongs to the train set. (0-2 points)**
```{r 'ch4-task4-3'}

# # number of rows in the Boston dataset 
# n <- nrow(boston_scaled)
# 
# # choose randomly 80% of the rows
# ind <- sample(n,  size = n * 0.8)
# 
# # create train set
# train <- boston_scaled[ind,]
# 
# # create test set 
# test <- boston_scaled[-ind,]
# 
# # save the correct classes from test data
# correct_classes <- test$crime
# 
# # remove the crime variable from test data
# test <- dplyr::select(test, -crime)
# 
```

## Linear Discriminant Analysis
**Fit the linear discriminant analysis on the train set. Use the categorical crime rate as the target variable and all the other variables in the dataset as predictor variables. Draw the LDA (bi)plot. (0-3 points)**
```{r 'ch4-task5'}
# # linear discriminant analysis
# lda.fit <- lda(crime ~ ., data = train)
# 
# # print the lda.fit object
# lda.fit
# 
# Function lifted directly from DataCamp exercises
# # the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}
# 
# # target classes as numeric
# classes <- as.numeric(train$crime)
# 
# # plot the lda results
# plot(lda.fit, dimen = 2, col=classes, pch=classes)
# lda.arrows(lda.fit, myscale = 1)
```

**Save the crime categories from the test set and then remove the categorical crime variable from the test dataset. Then predict the classes with the LDA model on the test data. Cross tabulate the results with the crime categories from the test set. Comment on the results. (0-3 points)**
```{r 'ch4-task6'}
# # predict classes with test data
# lda.pred <- predict(lda.fit, newdata = test)
# 
# # cross tabulate the results
# table(correct = correct_classes, predicted = lda.pred$class)
```
## k-means Clustering
**Reload the Boston dataset and standardize the dataset (we did not do this in the Datacamp exercises, but you should scale the variables to get comparable distances). Calculate the distances between the observations. Run k-means algorithm on the dataset. Investigate what is the optimal number of clusters and run the algorithm again. Visualize the clusters (for example with the pairs() or ggpairs() functions, where the clusters are separated with colors) and interpret the results. (0-4 points)**
```{r 'ch4-task7'}
# Set seed for reproducible results, k-means algorithm sets the initial
# cluster center points randomly.
set.seed(2020)

# Reload original Boston dataset
data('Boston')

# Scale the dataset
boston_scaled <- scale(Boston) %>% as.data.frame()
summary(boston_scaled)
# Calculate euclidean distance and show it
dist_eu <- dist(Boston)
summary(dist_eu)
 
# # manhattan distance matrix
# dist_man <- dist(Boston, method="manhattan")
# 
# # look at the summary of the distances
# summary(dist_man)
# # 

# k-means clustering
km <-kmeans(boston_scaled, centers = 3)
# 
# # plot the Boston dataset with clusters
# pairs(Boston[6:10], col = km$cluster)
# 

# # determine the number of clusters
k_max <- 10
# 
# # calculate the total within sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(boston_scaled, k)$tot.withinss})
# 
# # visualize the results
qplot(x = 1:k_max, y = twcss, geom = 'line')
# 
# # k-means clustering
# km <-kmeans(Boston, centers = 2)
# 
# # plot the Boston dataset with clusters
pairs(boston_scaled, col = km$cluster)
# 

```
## Bonus task
**Bonus: Perform k-means on the original Boston data with some reasonable number of clusters (> 2). Remember to standardize the dataset. Then perform LDA using the clusters as target classes. Include all the variables in the Boston data in the LDA model. Visualize the results with a biplot (include arrows representing the relationships of the original variables to the LDA solution). Interpret the results. Which variables are the most influencial linear separators for the clusters? (0-2 points to compensate any loss of points from the above exercises)**
```{r 'ch4-bonus'}
# Reload original Boston dataset
data('Boston')
# Scale the dataset
boston_scaled <- scale(Boston) %>% as.data.frame
summary(boston_scaled)
# # # k-means clustering
km <-kmeans(boston_scaled, centers = 7)
summary(km)
```

## Super-Bonus Task
**Super-Bonus: Run the code below for the (scaled) train data that you used to fit the LDA. The code creates a matrix product, which is a projection of the data points.**
```{r 'ch4-super-bonus-provided'}
# This script is provided in assignment #
# model_predictors <- dplyr::select(train, -crime)
# 
# # check the dimensions
# dim(model_predictors)
# dim(lda.fit$scaling)
# 
# # matrix multiplication
# matrix_product <- as.matrix(model_predictors) %*% lda.fit$scaling
# matrix_product <- as.data.frame(matrix_product)

# Next, install and access the plotly package. Create a 3D plot (Cool!) of the columns of the matrix product by typing the code below.
# plot_ly(x = matrix_product$LD1, y = matrix_product$LD2, z = matrix_product$LD3, type= 'scatter3d', mode='markers')
```

**Adjust the code: add argument color as a argument in the plot_ly() function. Set the color to be the crime classes of the train set. Draw another 3D plot where the color is defined by the clusters of the k-means. How do the plots differ? Are there any similarities? (0-3 points to compensate any loss of points from the above exercises)**
```{r 'ch4-super-bonus'}
```


```{r 'ch4-stash', eval=FALSE, include=FALSE}
# # calculate the correlation matrix and round it
# cor_matrix<-cor(Boston) 
# 
# # print the correlation matrix
# cor_matrix <- cor_matrix %>% round(digits=2)
# cor_matrix
# 
# # visualize the correlation matrix
# corrplot(cor_matrix, method="circle", type="upper",
# cl.pos="b", tl.pos="d", tl.cex=0.6)
# 
# 
# # MASS and Boston dataset are available
# 
# # center and standardize variables
# boston_scaled <- scale(Boston)
# 
# # summaries of the scaled variables
# summary(boston_scaled)
# 
# # class of the boston_scaled object
# class(boston_scaled)
# 
# # change the object to data frame
# boston_scaled <- as.data.frame(boston_scaled)
# 
# 
# # MASS, Boston and boston_scaled are available
# boston_scaled <- as.data.frame(scale(Boston))
# # summary of the scaled crime rate
# summary(boston_scaled)
# 
# # create a quantile vector of crim and print it
# bins <- quantile(boston_scaled$crim)
# bins
# 
# # create a categorical variable 'crime'
# crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, label=c("low", "med_low", "med_high", "high"))
# 
# # look at the table of the new factor crime
# table(crime)
# 
# # remove original crim from the dataset
# boston_scaled <- dplyr::select(boston_scaled, -crim)
# 
# # add the new categorical value to scaled data
# boston_scaled <- data.frame(boston_scaled, crime)
# 
# 
# 
# # boston_scaled is available
# 
# # number of rows in the Boston dataset 
# n <- nrow(boston_scaled)
# 
# # choose randomly 80% of the rows
# ind <- sample(n,  size = n * 0.8)
# 
# # create train set
# train <- boston_scaled[ind,]
# 
# # create test set 
# test <- boston_scaled[-ind,]
# 
# # save the correct classes from test data
# correct_classes <- test$crime
# 
# # remove the crime variable from test data
# test <- dplyr::select(test, -crime)
# 
# 
# # MASS and train are available
# 
# # linear discriminant analysis
# lda.fit <- lda(crime ~ ., data = train)
# 
# # print the lda.fit object
# lda.fit
# 
# # the function for lda biplot arrows
# lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
#   heads <- coef(x)
#   arrows(x0 = 0, y0 = 0, 
#          x1 = myscale * heads[,choices[1]], 
#          y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
#   text(myscale * heads[,choices], labels = row.names(heads), 
#        cex = tex, col=color, pos=3)
# }
# 
# # target classes as numeric
# classes <- as.numeric(train$crime)
# 
# # plot the lda results
# plot(lda.fit, dimen = 2, col=classes, pch=classes)
# lda.arrows(lda.fit, myscale = 1)
# 
# 
# # lda.fit, correct_classes and test are available
# 
# # predict classes with test data
# lda.pred <- predict(lda.fit, newdata = test)
# 
# # cross tabulate the results
# table(correct = correct_classes, predicted = lda.pred$class)
# 
# 
# # load MASS and Boston
# library(MASS)
# data('Boston')
# 
# # euclidean distance matrix
# dist_eu <- dist(Boston)
# 
# # look at the summary of the distances
# summary(dist_eu)
# 
# # manhattan distance matrix
# dist_man <- dist(Boston, method="manhattan")
# 
# # look at the summary of the distances
# summary(dist_man)
# 
# 
# # Boston dataset is available
# 
# # k-means clustering
# km <-kmeans(Boston, centers = 3)
# 
# # plot the Boston dataset with clusters
# pairs(Boston[6:10], col = km$cluster)
# 
# # MASS, ggplot2 and Boston dataset are available
# set.seed(123)
# 
# # determine the number of clusters
# k_max <- 10
# 
# # calculate the total within sum of squares
# twcss <- sapply(1:k_max, function(k){kmeans(Boston, k)$tot.withinss})
# 
# # visualize the results
# qplot(x = 1:k_max, y = twcss, geom = 'line')
# 
# # k-means clustering
# km <-kmeans(Boston, centers = 2)
# 
# # plot the Boston dataset with clusters
# pairs(Boston, col = km$cluster)
# 


```